% !TeX root = thoughts.tex
\section{Simple linear systems}

\subsection{Introduction with a 2 parameter system}
In a first analysis a simple linear system is considered, which allows us to get a grip on what is happening behind the scenes. A first analysis will be done on a two dimensional system given by the following linear system;
\begin{gather}\label{eq:linear_system.system}
	\begin{pmatrix}
	d_1\\
	d_2
	\end{pmatrix}
	=
	\begin{pmatrix}
	1 & 0\\
	0 & 2\\
	\end{pmatrix}
	\begin{pmatrix}
	q_1\\q_2
	\end{pmatrix}.
\end{gather}
For our synthetic data I choose $q_1 = 1$ and $q_2 = 3$. Prior information and measurement uncertainty all have an impact on posterior exploration, but for now I will set some arbitrary values. Our prior I synthetically set to have all means of 2 and standard deviations of 1. Measurement errors are chosen to be uncorrelated and having a standard deviation of 0.5. The influence of the two covariance matrices on the misfit functional are given in Figure~\ref{fig:linear_system.prior_influence}.

\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/linear_systems/gradient_2d_narrow_prior}
		\caption{Gradient with standard deviation $\sigma = 1$}
		\label{fig:linear_system.prior_influence.narrow}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/linear_systems/gradient_2d_wide_prior}
		\caption{Gradient with standard deviation $\sigma = 5$}
		\label{fig:linear_system.prior_influence.wide}
	\end{subfigure}
	\caption{Normalized gradient of 2D misfit functionals with differently confined prior information. The means of the prior are for both parameters 2. It is obvious that decreasing the standard deviation for all parameters means that we increase the importance of the prior over the data, `pulling' the minimal gradient more towards the prior mean. Similar effects can be attained with increasing the magnitude of the data covariance matrix. Together, these matrices finely tune the minimum of the misfit function. Choosing well informed data and parameter covariace matrices is essential to meaningful \gls{HMC} sampling. In this example, the data covariance was uniformly 0.5.}
	\label{fig:linear_system.prior_influence}
\end{figure}

\index{Trajectories}For illustration purposes, 50 time steps were taken with a stepsize of 0.05 in time units (actual units of time might be a bit meaningless, without defining the units of length and mass). Using the unit mass matrix now generates trajectories like the one described in Figure~\ref{fig:linear_system.trajectory_simple}. 

\begin{figure}
	\centering

	\includegraphics[width=0.5\textwidth]{figures/linear_systems/trajectory_simple}

	\caption{Untuned trajectory of the simple linear system described in Equation~\eqref{eq:linear_system.system}. The color of the trajectory points represents the normalized misfit, with red being the highest value, and dark blue being the lowest. Note that the direction of the trajectory can not be determined upon inspection of the trajectory itself; Hamilton's equation are invariant under time reversal. A particle would traverse exactly the same trajectory in reverse if at the end it's momentum would be reversed. The trajectory is superimposed on the gradient of the misfit functional $\chi$, which acts as the direction of the largest increase in gravitational potential. A particle starting without momentum would start to roll in opposite direction of these vectors, eventually orbiting the point of zero gradient (without energy loss it can never reach a steady state in the point of minimum energy).}
	\label{fig:linear_system.trajectory_simple}
\end{figure}	

\paragraph{Mass matrix optimization}\index{Mass matrix}\index{Trajectories!Tuning the}There are some undesirable characteristics of this trajectory. Due to the mass matrix being a diagonal unit matrix, but the misfit functional being elongated along the dimension of parameter $q_1$ the oscillations in either dimension do not have the same period. Using a mass matrix based on Equation~\eqref{eq:massMatrixForward} (which is still diagonal with the current forward model) will equalize oscillations. The reason this behavior is desired is that this way we see as many different energy levels in every dimension. The result of assigning the mass matrix based on the trace of $\MatrixVariable{G}^T\MatrixVariable{G}$ is seen in Figure~\ref{fig:linear_system.trajectory_massTuned}. The resulting mass matrix is ~
\begin{gather}\label{eq:linear_system.massMatrix}
\MatrixVariable{M} = 
\begin{pmatrix}
1 & 0\\
0 & 4\\
\end{pmatrix}.
\end{gather}

\begin{figure}
	\centering
	
	\includegraphics[width=0.5\textwidth]{figures/linear_systems/trajectory_massTuned}
	
	\caption{Trajectory tuned with mass matrix, according to the simple linear system described in Equation~\eqref{eq:linear_system.system}. The color of the trajectory points represents the normalized misfit, with red being the highest value, and dark blue being the lowest. With this augmented mass matrix, oscillations are equal in duration for each dimension.}
	\label{fig:linear_system.trajectory_massTuned}
\end{figure}

\paragraph{No U-Turn Criterion}\index{Trajectories!Trajectory length}\index{Trajectories!Tuning the}\index{No U-Turn Criterion}Visible now in Figure~\ref{fig:linear_system.trajectory_massTuned} is another characteristic of untuned trajectories. What would be ideal is that the algorithm explores the model space as efficiently as possible. The trajectory depicted in the figure traverses the model space around the minimum fully, but also start to come back to the original position. This so called `U-Turn' behavior can be mitigated by terminating the trajectory as soon as one detects the propagated model coming closer to the initial model. This point is reached as
\begin{gather}
	  \mathbf{v}(t) \cdot \left[ \mathbf{m}(t) - \mathbf{m}_0 \right] < 0, \quad
	 \text{and} \quad
	 \mathbf{v}_0\cdot \left[ \mathbf{m}_0 - \mathbf{m}(t) \right] < 0.
\end{gather}
This means that as soon as the momenta vectors for both the beginning and end of the trajectory both make an angle of less than 90 degrees with the vector connecting the points, so if the two models are `moving towards' each other, the trajectory is terminated. An illustration of a terminated trajectory is given in Figure~\ref{fig:linear_system.trajectory_uTurn}. By limiting the trajectories using this criterion, and additionally increasing the step size so only a few samples are needed to traverse to model space will effectively reduce computational costs of proposing new models.

\begin{figure}
	\centering
	
	\includegraphics[width=0.5\textwidth]{figures/linear_systems/trajectory_uTurn_29_samples}
	
	\caption{Trajectory tuned with the \textbf{No U-Turn Criterion} and forward model based mass matrix. The color of the trajectory points represents the normalized misfit, with red being the highest value, and dark blue being the lowest. As soon as the momentum of the initial point and the last point of the trajectory point `towards' eachother the trajectory is terminated. In this case, 29 samples were made before the trajectory was terminated.}
	\label{fig:linear_system.trajectory_uTurn}
\end{figure}

\paragraph{Stepsize of the trajectories}\index{Trajectories!Tuning the}\index{Trajectories!Stepsize} The step size is another important tuning parameter. Wasting many computation on many steps is wasteful if the `particle' will end up in the same place, as was done in previous illustrations. According to \cite{neal2011mcmc} the maximum step size is defined by the minimum standard deviation of momentum, or square root of the mass matrix.

\paragraph{Results}
Using optimized parameters, given in Table~\ref{tab:naive_params}, multiple inversions were performed using \gls{HMC}. The used stepsize is well below the upper limit of stability predicted by the mass matrix, according to \cite{neal2011mcmc}. As can be seen in Figure~\ref{fig:linear_system.accepted}, over many iterations the amount of accepted models evens out to approximately 66\%. I'll call this method `naive' for now, for reasons which will become apparent later.

\begin{table}[]
\centering
\begin{tabular}{l|l}
 Number of timesteps & 50 \\
 Length of timestep & 0.05 \\
 Predicted stability &  0.5
\end{tabular}
\caption{Inversion parameters for `naive' trajectory settings.}
\label{tab:naive_params}
\end{table}

In Figure~\ref{fig:linear_system.histq2} one sees what we'd expect to see with a linear model with Gaussian uncertainties. After more and more iterations the marginal posterior probability density functions approaches a normal distribution. It's actual mean in the last iteration is $\mu_1 = 2.941$, close to the original parameter. This value is reached to within 1\% as soon as 500 iterations. The actual development of the value is given in Figure~\ref{fig:linear_system.evolution_q2}. This all seems like ideal behaviour, our sampler is able to get meaningful statistic from our dataset after only 500 iterations, and after 50,000 iterations we have a very nice looking posterior.

When one however examines not only the marginal posterior of parameter 2, but also the marginal posterior of parameter 1 (Figure~\ref{fig:linear_system.histq1}) and the full two dimensional posterior (Figure~\ref{fig:linear_system.hist2d}) unexpected characteristics are found. It seems that parameter 1 has a bimodal distribution, while none of our input prior information nor forward model suggests such behaviour.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/linear_systems/2d_naive/accepted.pdf}
	\caption{Acceptance rates of the 'naive' sampling.}
	\label{fig:linear_system.accepted}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/linear_systems/2d_naive/histogram_p2.pdf}
	\caption{Histogram for marginal probability density of parameter 2 after `naive' HMC sampling using different amount of
	samples.}
	\label{fig:linear_system.histq2}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{figures/linear_systems/2d_naive/means2.pdf}
	\caption{Evolution of mean of parameter 2 as the number of samples increases during the `naive' sampling.}
	\label{fig:linear_system.evolution_q2}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/linear_systems/2d_naive/histogram_p1.pdf}
	\caption{Histogram for marginal probability density of parameter 1 after `naive' HMC sampling using different amount of
	samples.}
	\label{fig:linear_system.histq1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/linear_systems/2d_naive/histogram_2d.pdf}
	\caption{Full 2D marginal probability density after `naive' HMC sampling using different amount of
	samples.}
	\label{fig:linear_system.hist2d}
\end{figure}

\afterpage{\clearpage}

Upon closer inspection of the random walk trajectory done after 50,000 iterations (Figure~\ref{fig:linear_system.randomWalk50000naive}), there seems to be some bias in proposing new models. What happened in this case which spefically leads to the bimodal distribution is theorized as follows; the prior information always leads to propose the first model close to $\mathbf{q} = (2,2)^T$. This starting point coupled with the rather long trajectory length of 50 samples and still rather large stepsize, result in every iteration traversing almost the complete model space.

This, in addition with the No U-Turn Criterion, makes these trajectories `resonate', always exploring the same portion of the model space. If a model is launched away from the misfit minimum, it wil return towards its original position and then be killed by the No U-Turn Criterion. If it is launched in a relatively beneficial direction. it will traverse a lot of space and turn at the opposite side of the model space. I would not go as far as to call it \index{Hysteresis}hysteresis, but rather a fundamental result of the \index{Tuning parameters}tuning parameters.

There's two ways to test this hypothesis. First, the random walk using a different prior is analysed. Now the prior means are chosen at exactly the values of the parameters used for the synthetics. The result, given in Figure~\ref{fig:linear_system.improvedmean}, is that sampling is a little bit improved. The sampling is more uniform in the 2D space, but there is still some kind of `orbiting' behaviour, the sampler never reaches the minimum potential. This is because all trajectories passing through this point are not yet terminated. Making trajectories even longer won't work, because the No U-Turn Criterion will terminate them anyway. This leads us to testing another method of avoiding \index{Biased sampling}biased sampling; shorter trajectories.

\index{Trajectories!Trajectory length}I think that this is a better solution. By decreasing total trajectory length, one does not need to tune actual physical intuition or information (the prior) to the needs of the algorithm. Recognizing this behaviour in non-linear or high dimensional systems might prove very challenging upon inspection of marginal probability distributions. One quality check one could do, even during sampling, is to monitor how often the No U-Turn Criterion triggers the termination of a trajectory, relative to total models ran. The promising result is given in Figure~\ref{fig:linear_system.improvedtrajectory}.

\begin{figure}
	\centering
	\includegraphics[width=0.66\textwidth]{figures/linear_systems/2d_naive/randomWalk_50000.png}
	\caption{Random walk during 50,000 `naive' \gls{HMC} samples.}
	\label{fig:linear_system.randomWalk50000naive}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{.43\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/linear_systems/2d_improved/mean_histogram_2d}
		\caption{Full 2D posterior}
		\label{}
	\end{subfigure}%
	\begin{subfigure}{.57\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/linear_systems/2d_improved/mean_randomWalk}
		\caption{Random walk sequence}
		\label{}
	\end{subfigure}
	\caption{50,000 samples of the same 2D posterior as Figure~\ref{fig:linear_system.hist2d}, tuned with a different starting prior. Note that the random walk still traverse most of the model space in each proposal, leading again to `orbiting' of the minimum.}
	\label{fig:linear_system.improvedmean}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{.43\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/linear_systems/2d_improved/short_histogram_2d}
		\caption{Full 2D posterior}
		\label{}
	\end{subfigure}%
	\begin{subfigure}{.57\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/linear_systems/2d_improved/short_randomWalk}
		\caption{Random walk sequence}
		\label{}
	\end{subfigure}
	\caption{50,000 samples of the same 2D posterior as Figure~\ref{fig:linear_system.hist2d}, tuned with a much smaller trajectory length. Note that now each new model only has a small distance from the previous, but that the model space is much more evenly sampled.}
	\label{fig:linear_system.improvedtrajectory}
\end{figure}

\subsection{Performance of higher dimensional systems}

In this section I want to look at the influence of dimensionality; how it affects computation time and acceptance rates. A general forward model is presented for $n$-dimensions;

\begin{gather}
	\MatrixVariable{G} =
	\begin{bmatrix}
		1\cdot 1 &   &        & \\
		  & 2\cdot 2 &        & \\
		  &   & \ddots & \\
		  &   &        & n \cdot n
	\end{bmatrix}
\end{gather}
\paragraph{A note on the mass matrix}\index{Mass matrix!revised}The mass matrix was initially again based upon the trace of $\MatrixVariable{G}^T\MatrixVariable{G}$. This \textbf{did not} produce equally oscillating trajectories with models larger than 2 dimension and varying forward models. After some trial and error, and revisiting the theory, an extension upon the mass matrix is made. In the aforementioned reader, the covariance matrices were not analyzed in the general solution. By substituting the `new' $\MatrixVariable{A}$ and $\mathbf{b}$ from equations \eqref{eq:linear_system.misfit_A} and \eqref{eq:linear_system.misfit_b}, we see that a more optimized mass matrix could be constructed from (the trace of) $\MatrixVariable{A}$. This deficit was not apparent in the analysis of the 2 dimensional system probably due to the small difference in forward matrix components and parameters.

\paragraph{Performance increase}\index{Algorithm performance}At this stage I also realized that there are two main points on which we can increase performance. The code at this stage was sped up threefold by compiling using any of the -O\# g++ flags. Any specific compiler flag (-O1 through -O4) didn't noticeably alter performance, mostly invisible due to the random nature of the sampling. I advise therefore to use -O2, the most tested mode, which alters your code the least. Also, RAM usage can be easily decreased by limiting the amount of std::vector$<>$ copies. Every time one of those objects is passed and used only once, (near) perfect forwarding can be achieved using std::move().


\paragraph{Inversion characteristics}The step size of any model is fixed at 0.05, since the first dimension provides the upper bound to the stability. For the trajectory, 10 different iterations are done, or whenever the U-Turn criterion is met. In total, 50,000 samples are drawn. Prior information for every parameter is set at 10, with a standard deviation of 5. This means that as the dimensionality increases, the prior becomes worse with respect to the actual parameter. Data covariance is fixed at 10 percent of the observed value, as to negate it's influence on the algorithm as much as possible. 

\paragraph{Results}\index{Algorithm performance!runtime}The computation time versus number of parameters exhibits an almost linear relationship, as seen in Figure~\ref{fig:linear_system.high_dim.time}. The acceptance rate seems to suffer from the increasing dimensions, appearing inversely proportional to the amount of parameters (Figure~\ref{fig:linear_system.high_dim.acceptance}). The results are rather good, but decreasing in quality as the number of accepted models decreases. This performance might be improved by actually drawing correlated samples from the mass matrix (I advise Cholesky decomposition).

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/linear_systems/higher_dim/performance}
	\caption{Computation time of 50,000 samples for a varying amount of parameters.}
	\label{fig:linear_system.high_dim.time}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/linear_systems/higher_dim/acceptance_rate}
	\caption{Acceptance rate of 50,000 samples for a varying amount of parameters.}
	\label{fig:linear_system.high_dim.acceptance}
\end{figure}









